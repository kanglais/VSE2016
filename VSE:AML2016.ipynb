{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 training images\n",
      "Clustering...\n",
      "(100, 128)\n"
     ]
    }
   ],
   "source": [
    "#get SIFT points, cluster the data\n",
    "\n",
    "from sklearn.cluster import k_means\n",
    "\n",
    "def cluster_data(features, k, nr_iter=50):\n",
    "    #Just a wrapper about sklearn\n",
    "    centroids = k_means(features, n_clusters=k, max_iter=nr_iter)[0]\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "# Define the number of clusters.\n",
    "nr_clusters = 100\n",
    "\n",
    "\n",
    "# Load the file containing the training images.\n",
    "trainimages = [line.strip().split(\" \")[0] for line in open(\"trainset-overview.txt\", \"r\")]\n",
    "print \"There are\", len(trainimages), \"training images\"\n",
    "\n",
    "trainpoints = []\n",
    "\n",
    "for i in xrange(len(trainimages)):   \n",
    "    # Extract point locations from the image using your selected point method and parameters.\n",
    "    image_name = trainimages[i]\n",
    "\n",
    "    densePoints = sift.densePoints(image_name, stride=25)\n",
    "    sigma       = 1.0\n",
    "    hesPoints   = sift.computeHes(image_name, sigma, magThreshold=15, hesThreshold=10, NMSneighborhood=10)\n",
    "    harPoints   = sift.computeHar(image_name, sigma, magThreshold=5, NMSneighborhood=10)\n",
    "\n",
    "    # Compute the SIFT features.\n",
    "    allpoints = np.concatenate((densepoints, hespoints, harpoints))\n",
    "    \n",
    "    point1, sift1 = sift.computeSIFTofPoints(image_name, allpoints, sigma, nrOrientBins=8, nrSpatBins=4, nrPixPerBin=4)\n",
    "    \n",
    "    trainpoints.append(sift1)\n",
    "\n",
    "trainsift = np.vstack(trainpoints)\n",
    "    \n",
    "    \n",
    "# Cluster the SIFT features and put them in a matrix with the name 'clusters'!\n",
    "\n",
    "print \"Clustering...\"\n",
    "clusters = cluster_data(trainsift, nr_clusters)\n",
    "print clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "#make bag-of-words histogram of all the images\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Length of histogram vector.\n",
    "size_of_histograms = 50\n",
    "\n",
    "train_feat = np.zeros((len(trainimages), size_of_histograms))\n",
    "\n",
    "# Go through the SIFTs of every image and create a histogram for the image\n",
    "# relative to the clusters you discovered in the previous phase.\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    assert(len(x) == len(y))\n",
    "    d = 0.0\n",
    "    \n",
    "    nr_dimensions = len(x)\n",
    "        \n",
    "    d = np.sum((x-y)**2)\n",
    "    \n",
    "    d = np.sqrt(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def distances(a,X,distance_fn=euclidean_distance):\n",
    "    dists = np.zeros(X.shape[0])\n",
    "    \n",
    "    for i,v in enumerate(X): \n",
    "        dstnc = distance_fn(a,v)  \n",
    "        dists[i] = dstnc\n",
    "            \n",
    "    return dists\n",
    "\n",
    "def cluster_assignment(samples, clusters):\n",
    "    nr_samples = samples.shape[0]\n",
    "    \n",
    "    nr_clusters = clusters.shape[0]\n",
    "    \n",
    "    assignments  = np.zeros(nr_samples, dtype=int)\n",
    "    \n",
    "    for i in range(samples.shape[0]):        \n",
    "        assignments[i] = np.argmin(distances(samples[i], clusters))\n",
    "\n",
    "    \n",
    "    return assignments\n",
    "\n",
    "\n",
    "def create_histogram(samples, clusters):\n",
    "    # Perform the assignments first.\n",
    "    assignments = cluster_assignment(samples,clusters)\n",
    "    \n",
    "    # Initialize the histogram.\n",
    "    histogram   = np.zeros(clusters.shape[0], dtype=np.float)\n",
    "    \n",
    "    # Go over all the assignments and place them in the correct histogram bin.     \n",
    "    for assignment in assignments: \n",
    "        histogram[assignment] +=1\n",
    "   \n",
    "    # Normalize the histogram such that the sum of the bins is equal to 1.\n",
    "    histogram = normalize(histogram.reshape(1,-1), norm='l1')[0]\n",
    "    \n",
    "    return histogram\n",
    "\n",
    "\n",
    "def images_histogram(list_of_images, clusters):\n",
    "\n",
    "    image_sifts = []\n",
    "\n",
    "    #len(list_of_images)\n",
    "    for i in xrange(len(list_of_images)):\n",
    "    \n",
    "        image_name = list_of_images[i]\n",
    "        \n",
    "        densePoints = sift.densePoints(image_name, stride=5)\n",
    "\n",
    "        sigma       = 1.0\n",
    "        hesPoints   = sift.computeHes(image_name, sigma, magThreshold=15, hesThreshold=10, NMSneighborhood=10)\n",
    "\n",
    "        harPoints   = sift.computeHar(image_name, sigma, magThreshold=10, NMSneighborhood=10)\n",
    "\n",
    "        allpoints = np.concatenate((densepoints, hespoints, harpoints))\n",
    "        \n",
    "        point1, sift1 = sift.computeSIFTofPoints(image_name, allpoints, sigma, nrOrientBins=8, nrSpatBins=4, nrPixPerBin=4) \n",
    "        \n",
    "        image_cluster_sifts = create_histogram(sift1, clusters)\n",
    "        \n",
    "        image_sifts.append(image_cluster_sifts)\n",
    "\n",
    "    image_sifts = np.vstack(image_sifts)\n",
    "    \n",
    "    return image_sifts\n",
    "\n",
    "image_sifts = images_histogram(trainimages, clusters)\n",
    "\n",
    "print image_sifts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#next few cells: train the classifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Training ground truth labels\n",
    "train_labels = np.array([int(line.strip().split(\" \")[1]) for line in open(\"trainset-overview.txt\", \"r\")])\n",
    "\n",
    "#Validation images\n",
    "valimages = [line.split(' ')[0] for line in open('valset-overview.txt','r')]\n",
    "\n",
    "#Validation ground truth labels\n",
    "val_labels = np.array([int(line.rstrip().split(' ')[1]) for line in open('valset-overview.txt','r')])\n",
    "\n",
    "#To do by you:\n",
    "#Calculate the histogram representations for the validation images\n",
    "\n",
    "histo_val_img = images_histogram(valimages, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "#SVM classifier: find best C value\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def validate_svm_parameter(trainx, trainy, valx, valy):\n",
    "    cs = np.logspace(-2,+2,5)\n",
    "    bestc = 0\n",
    "    scores = []\n",
    "    \n",
    "    # To do by you.\n",
    "    \n",
    "    for element in range(0,len(cs)):\n",
    "        clf = LinearSVC(C = cs[element])\n",
    "        clf.fit(trainx, trainy)\n",
    "        trainx.shape, trainy.shape\n",
    "        score = clf.score(valx, valy)\n",
    "        scores.append(score)\n",
    "    \n",
    "    max_val = np.argmax(scores)\n",
    "    bestc = cs[max_val]\n",
    "    \n",
    "    # Return best c value.\n",
    "    return bestc\n",
    "        \n",
    "# Call the function.\n",
    "bestc = validate_svm_parameter(image_sifts, train_labels, histo_val_img, val_labels)\n",
    "\n",
    "print bestc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.37\n"
     ]
    }
   ],
   "source": [
    "#SVM classifier: train classifier, find accuracy (score)\n",
    "\n",
    "clf = LinearSVC(C=10)\n",
    "clf.fit(image_sifts, train_labels)\n",
    "\n",
    "print clf \n",
    "\n",
    "\n",
    "score = clf.score(histo_val_img, val_labels)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 0, 0, 0, 5, 0, 0, 9, 5, 6, 0, 0, 3, 0, 0, 6, 5, 0, 6, 6, 9, 1, 7, 3, 2, 8, 1, 7, 7, 1, 4, 6, 3, 8, 1, 4, 1, 4, 1, 7, 2, 4, 8, 4, 7, 5, 1, 6, 9, 4, 4, 2, 4, 4, 9, 4, 7, 3, 7, 2, 3, 3, 7, 1, 3, 3, 7, 6, 3, 4, 4, 4, 4, 7, 6, 3, 8, 3, 3, 1, 4, 4, 7, 1, 3, 3, 4, 7, 4, 4, 3, 4, 3, 3, 4, 7, 6, 6, 1, 2, 5, 9, 0, 6, 5, 5, 3, 6, 5, 5, 6, 6, 0, 7, 3, 5, 6, 3, 9, 0, 6, 8, 6, 2, 8, 6, 6, 3, 6, 1, 0, 3, 4, 1, 6, 5, 1, 1, 0, 5, 1, 9, 7, 3, 7, 7, 9, 7, 1, 7, 7, 7, 1, 3, 7, 1, 4, 7, 7, 9, 3, 4, 4, 1, 3, 4, 4, 1, 7, 8, 4, 4, 8, 9, 3, 2, 3, 8, 3, 4, 9, 9, 1, 9, 9, 9, 5, 3, 7, 9, 5, 5, 9, 9, 6, 9, 7, 9, 3, 9]\n"
     ]
    }
   ],
   "source": [
    "#KNN classifier: Euclidean distance, then predict labels of validation set\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def knn(data_point,train_data,train_label,k):\n",
    "\n",
    "    d = distances(data_point, train_data)\n",
    "    labels = train_label[np.argsort(d)[0:k]]\n",
    "    prediction = Counter(labels).most_common()[0][0]\n",
    "    return prediction\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for i in xrange(len(histo_val_img)):\n",
    "    predicted_labels.append(knn(histo_val_img[i], image_sifts, train_labels, 50))\n",
    "\n",
    "print predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=20,\n",
      "            max_features=100, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.255\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier, and validation prediction/ score. Call all tree/ forest/ ada classifiers from sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth=20, max_features=100)\n",
    "dtree = dtree.fit(image_sifts, train_labels)\n",
    "\n",
    "print dtree\n",
    "\n",
    "decisionscore = dtree.score(histo_val_img, val_labels) # score: 0.245\n",
    "\n",
    "print decisionscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.365\n"
     ]
    }
   ],
   "source": [
    "#Random forest classifier, r.forest prediction\n",
    "\n",
    "rtree = RandomForestClassifier(max_depth=20, n_estimators=300)\n",
    "rtree = rtree.fit(image_sifts, train_labels)\n",
    "\n",
    "print rtree\n",
    "\n",
    "randomscore = rtree.score(histo_val_img, val_labels) #score: .4\n",
    "\n",
    "print randomscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.25, n_estimators=1000, random_state=None)\n",
      "0.26\n"
     ]
    }
   ],
   "source": [
    "#adaboost classifier, adaboost prediction\n",
    "\n",
    "ada = AdaBoostClassifier(learning_rate=0.25, n_estimators=1000)\n",
    "ada = ada.fit(image_sifts, train_labels)\n",
    "\n",
    "print ada\n",
    "\n",
    "score = ada.score(histo_val_img, val_labels)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Phase 5: Predicting test images and uploading the results</h3>\n",
    "<p>\n",
    "Once you have a model that works well on the validation set, you can submit your predictions on the test set. We provide you with the image filename list as well as code to save your predictions as a csv so you can submit to Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict test images; upload the results\n",
    "\n",
    "testimages = [line.strip().split(' ')[0] for line in open('testset-overview-final.txt','r')]\n",
    "\n",
    "test_img_histo = images_histogram(testimages, clusters)\n",
    "\n",
    "#Use classifier to make class predictions:\n",
    "test_predictions = np.zeros(len(testimages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNN test predictions\n",
    "for i in xrange(len(test_img_histo)):\n",
    "    test_predictions[i] = (knn(test_img_histo[i], image_sifts, train_labels, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Decision tree test predictions\n",
    "test_predictions = dtree.predict(test_img_histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RandomForest test predictions\n",
    "test_predictions = rtree.predict(test_img_histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AdaBoost test predictions\n",
    "test_predictions = ada.predict(test_img_histo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 9 6 1 3 0 4 4 1 4 2 4 6 0 6 0 1 9 0 7 2 0 9 9 2 0 1 5 4 1 5 9 9 2 8 5 0\n",
      " 4 8 2 5 5 1 1 8 0 2 0 2 1 6 6 4 3 5 8 9 5 4 7 4 3 2 3 3 5 0 1 5 6 1 1 5 7\n",
      " 0 9 7 4 2 9 3 3 2 4 9 1 9 2 6 2 9 9 2 4 2 9 0 0 8 1 4 5 1 3 5 2 6 8 2 6 4\n",
      " 9 8 2 6 9 7 0 3 5 1 2 1 7 5 5 0 5 0 1 9 3 3 0 9 2 0 2 0 9 0 7 7 5 2 5 2 7\n",
      " 7 0 8 6 2 7 0 4 4 2 1 0 4 3 4 9 1 3 0 7 1 5 2 1 6 2 1 7 2 0 4 8 0 3 6 3 0\n",
      " 4 0 1 1 7 3 6 2 4 0 8 0 6 4 8 5 2 3 9 8 4 7 0 8 2 5 8 2 0 5 9 4 6 9 9 9 1\n",
      " 8 5 1 1 0 3 0 8 0 4 1 1 0 6 1 3 6 4 7 2 3 6 6 3 1 5 9 4 1 9 6 4 1 5 2 5 1\n",
      " 1 9 8 5 6 9 9 3 5 8 7 3 1 3 4 2 3 4 0 0 4 0 0 7 2 5 4 4 0 1 7 3 2 0 0 5 1\n",
      " 4 4 5 3 1 5 9 3 4 4 2 2 7 9 2 5 6 0 2 9 9 2 0 3 9 0 7 1 1 0 1 9 4 0 6 9 7\n",
      " 0 0 2 5 9 6 3 1 1 3 1 0 5 3 0 1 4 0 9 8 1 3 9 5 4 1 2 0 6 4 2 7 1 6 0 4 4\n",
      " 0 0 6 4 1 0 2 9 5 8 0 1 0 3 6 1 5 9 4 7 1 4 3 5 9 0 0 4 9 1 5 0 2 6 2 1 4\n",
      " 6 2 5 6 2 4 0 8 3 6 1 4 8 4 6 4 1 2 9 5 6 1 9 6 8 2 3 6 4 1 5 2 3 0 4 3 0\n",
      " 7 5 9 2 4 8 2 2 1 2 5 0 2 1 1 6 4 8 2 6 2 2 9 9 4 8 9 8 6 9 2 4 0 0 1 4 9\n",
      " 9 9 5 2 4 0 0 7 2 5 6 4 5 4 5 9 4 8 7]\n"
     ]
    }
   ],
   "source": [
    "#SVM test predictions\n",
    "cs = np.logspace(-2,+2,5)\n",
    "\n",
    "for element in range(0,len(cs)):\n",
    "        clf = LinearSVC(C = cs[element])\n",
    "        clf.fit(image_sifts, train_labels)\n",
    "        image_sifts.shape, train_labels.shape\n",
    "        test_predictions = clf.predict(test_img_histo)\n",
    "   \n",
    "test_predictions = clf.predict(test_img_histo)\n",
    "    \n",
    "print test_predictions\n",
    "\n",
    "#We save your predictions to file\n",
    "test_p_file = open('test_predictions_bad.csv','w')\n",
    "test_p_file.write('ImageName,Prediction\\n')\n",
    "for i,image in enumerate(testimages):\n",
    "    test_p_file.write(image+','+str(int(test_predictions[i]))+'\\n')\n",
    "test_p_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your predictions have been saved to a file called 'test_predictions.csv'. You can upload this through the Kaggle interface to make your submission.\n",
    "<p><h3>Want to do better?</h3>\n",
    "In order to get the best performance, you will  need to tune the parameters of SIFT, of the bag-of-words (number of clusters), and of your classifier.\n",
    "<br>\n",
    "<br>\n",
    "If you think that you've exhausted these parameters but still want to perform better, you can look at a number of things:\n",
    "<ul>\n",
    "<li>What if I use other distance functions in my knn classifiers? Look e.g. at: the histogram intersection distance or Manhattan distance online.</li>\n",
    "<li>Other classifiers: You are allowed to use other classifiers from sklearn (e.g. SVMs, Random Forests, AdaBoost, ...). You will need to figure out how these functions work and how to set the parameters (using the validation images). You should also be prepared to provide the motivation for any decisions in your report.</li>\n",
    "<li>What about better bag-of-words representations (e.g. VLAD or Fisher Vector)?</li>\n",
    "<li>What about deep networks?</li>\n",
    "<li>Your own weird idea. If you have some idea in your head, try it out! We welcome those ideas. They do not even have to give a  performance boost, as long as you can explain the reasons behind your idea.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
